{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\nKUL H02A5a Computer Vision: Group Assignment 1\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r0585077, r0595762, r0654230, r0738552, r0829665</span>.\n\nIn this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 0* and you start from this template notebook. The notebook you submit for grading is the last notebook you submit in the [Kaggle competition](https://www.kaggle.com/t/b7a2a8743bd842ca9ac93ae91cbc8d9f) prior to the deadline on **Tuesday 18 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 1* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n\n---------------------------------------------------------------\nNOTES:\n* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n","metadata":{"_cell_guid":"b47b15de-64a5-4fa9-a688-23d3efa9a2f4","_uuid":"0cc385a7-98f6-4883-96eb-7b89c7c9aa1c","papermill":{"duration":0.010809,"end_time":"2021-03-29T09:04:17.390934","exception":false,"start_time":"2021-03-29T09:04:17.380125","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Overview\nThis assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n* Image classification (Sect. 2)\n* Semantic segmentation (Sect. 3)\n* Adversarial attacks (Sect. 4)\n\nIn the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook.","metadata":{"_cell_guid":"35358cfb-b13d-4277-8dd5-4e663c8cd775","_uuid":"3b40b846-d7da-46d8-b354-c6d5c5ded56e","papermill":{"duration":0.009414,"end_time":"2021-03-29T09:04:17.410239","exception":false,"start_time":"2021-03-29T09:04:17.400825","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1.1 Deep learning resources\nIf you did not yet explore this in *Group assignment 0 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO).","metadata":{"papermill":{"duration":0.009248,"end_time":"2021-03-29T09:04:17.429068","exception":false,"start_time":"2021-03-29T09:04:17.41982","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -U segmentation-models-pytorch albumentations --user \n!pip install albumentations==0.5.2\nimport os\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,2,3,4\"\n\n\nimport sys\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, log_loss, confusion_matrix\nimport albumentations as albu\nimport pickle\nprint(albu.__version__)","metadata":{"_cell_guid":"7ddf657a-b938-4a49-87dc-b0db9af9156d","_uuid":"c65ea4f1-cc90-408f-b8e0-7c7399ec7e21","papermill":{"duration":6.498173,"end_time":"2021-03-29T09:04:23.936979","exception":false,"start_time":"2021-03-29T09:04:17.438806","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-05T14:31:15.756025Z","iopub.execute_input":"2021-06-05T14:31:15.756349Z","iopub.status.idle":"2021-06-05T14:31:33.85155Z","shell.execute_reply.started":"2021-06-05T14:31:15.756317Z","shell.execute_reply":"2021-06-05T14:31:33.850107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 PASCAL VOC 2009\nFor this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes.","metadata":{"papermill":{"duration":0.009505,"end_time":"2021-03-29T09:04:23.956792","exception":false,"start_time":"2021-03-29T09:04:23.947287","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:33.853243Z","iopub.execute_input":"2021-06-05T14:31:33.853549Z","iopub.status.idle":"2021-06-05T14:31:40.806572Z","shell.execute_reply.started":"2021-06-05T14:31:33.85352Z","shell.execute_reply":"2021-06-05T14:31:40.804565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the training data\ntrain_df = pd.read_csv('/kaggle/input/kul-h02a5a-computervision-groupassignment1/train/train_set.csv', index_col=\"Id\")\nlabels = train_df.columns[:]\nprint(\"The training set contains {} examples.\".format(len(train_df)))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:40.808131Z","iopub.execute_input":"2021-06-05T14:31:40.808542Z","iopub.status.idle":"2021-06-05T14:31:40.831696Z","shell.execute_reply.started":"2021-06-05T14:31:40.8085Z","shell.execute_reply":"2021-06-05T14:31:40.830854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train_df.columns[:]\nprint(labels)\ntrain_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:40.832975Z","iopub.execute_input":"2021-06-05T14:31:40.833345Z","iopub.status.idle":"2021-06-05T14:31:40.855834Z","shell.execute_reply.started":"2021-06-05T14:31:40.833303Z","shell.execute_reply":"2021-06-05T14:31:40.854632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the test data\ntest_df = pd.read_csv('/kaggle/input/kul-h02a5a-computervision-groupassignment1/test/test_set.csv', index_col=\"Id\")\n#test_df[\"img\"] = [np.load('/content/drive/My Drive/CV Assignment 2/Data/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n#test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\nprint(\"The test set contains {} examples.\".format(len(test_df)))\n\n# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\ntest_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:40.858731Z","iopub.execute_input":"2021-06-05T14:31:40.859161Z","iopub.status.idle":"2021-06-05T14:31:40.8849Z","shell.execute_reply.started":"2021-06-05T14:31:40.859117Z","shell.execute_reply":"2021-06-05T14:31:40.883917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup path variables\nDATA_DIR = '/kaggle/input/kul-h02a5a-computervision-groupassignment1'\nx_train_dir = os.path.join(DATA_DIR,'train', 'img')\ny_train_dir = os.path.join(DATA_DIR,'train', 'seg')\n\nprint(x_train_dir)\n\ndo_test_split=False\n\nif not do_test_split:\n  x_test_dir = os.path.join(DATA_DIR,'test', 'img')\n  y_test_dir = None\nelse:\n  x_test_dir = x_train_dir \n  y_test_dir = y_train_dir","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:40.886379Z","iopub.execute_input":"2021-06-05T14:31:40.886765Z","iopub.status.idle":"2021-06-05T14:31:40.894635Z","shell.execute_reply.started":"2021-06-05T14:31:40.886725Z","shell.execute_reply":"2021-06-05T14:31:40.892133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization","metadata":{}},{"cell_type":"code","source":"from skimage.color import label2rgb\nimport skimage\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as mpatches\nprint(skimage.__version__)\nCOLORS=('red', 'blue', 'yellow', 'magenta', \n            'green', 'indigo', 'darkorange', 'cyan', 'pink', \n            'yellowgreen', 'darkmagenta', 'brown',\n            'purple', 'darkviolet',\"dodgerblue\",\n            \"darkgoldenrod\",\"lawngreen\",\"crimson\",\n              \"darkkhaki\",\"darkgreen\")\n\n\n\n# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n\n\n# helper function for data visualization\ndef plot_mask(img,mask,mask_truth=None,score=None,labels=labels):\n  # create colormap for the legend\n  cmap1 = ListedColormap(COLORS).colors\n  cl_nums1 = np.unique(mask)\n  cl_nums1=cl_nums1[cl_nums1!=0]\n  # Subtract one so it can act as index for the labels\n  cl_nums1= [x - 1 for x in cl_nums1]\n  # Subset the list of all colors to bind color to class, instead of cycling\n  # through colors \n  cmap1 = [cmap1[i] for i in cl_nums1]\n  # create empty colored rectangles labeled with the segmentation class names\n  # for the legend \n  r1 = [mpatches.Rectangle((0,0), 1, 1, color=cmap1[i],label=labels[cl_nums1[i]]) for i in range(len(cl_nums1))]\n  result1 = label2rgb(mask,img,alpha=0.5,bg_label=0,colors=cmap1,kind=\"overlay\")\n  if mask_truth is None:\n    plt.figure(figsize=(16, 5))\n    # Mask input image with binary mask\n    plt.imshow(result1)\n    if score is not None:\n      plt.title.set_text(\"IoU=\"+str(score))\n    plt.legend(handles=r1, loc=4, borderaxespad=0.)\n    plt.show()\n  else:\n    # repeat for ground truth mask\n    cmap2 = ListedColormap(COLORS).colors\n    cl_nums2 = np.unique(mask_truth)\n    cl_nums2=cl_nums2[cl_nums2!=0]\n    cl_nums2= [x - 1 for x in cl_nums2]\n    cmap2 = [cmap2[i] for i in cl_nums2]\n    r2 =  [mpatches.Rectangle((0,0), 1, 1, color=cmap2[i],label=labels[cl_nums2[i]]) for i in range(len(cl_nums2))]\n    result2 = label2rgb(mask_truth,img,alpha=0.5,bg_label=0,colors=cmap2,kind=\"overlay\")\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    axes[0].imshow(result1)\n    if score is not None:\n      axes[0].title.set_text(\"IoU=\"+str(score))\n    axes[0].legend(handles=r1, loc=4, borderaxespad=0.)\n    axes[1].imshow(result2)\n    axes[1].legend(handles=r2, loc=4, borderaxespad=0.)\n\n\ndef plot_confusion_matrix(y_true, y_pred):\n  \"\"\"Plots a confusion matrix given true and predicted labels. \n  Parameters\n  ----------\n  y_true : array\n      True labels\n  y_pred : array\n      Predicted labels\n  \"\"\"\n\n# compute confusion matrix from input true and prediction labels\n  cmx = confusion_matrix(y_true, y_pred)\n  \n  # initialize plot, pass in confusion matrix, set labels on x and y axis\n  fig, ax = plt.subplots(1,1)\n  img = ax.imshow(cmx)\n  label_list = ['0', '1', '2']\n  ax.set_xticks([0,1,2])\n  ax.set_xticklabels(label_list)\n  ax.set_yticks([0,1,2])\n  ax.set_yticklabels(label_list)\n  plt.title('Confusion Matrix')\n  plt.xlabel('True Labels')\n  plt.ylabel('Predicted Labels')\n  \n  # include numbers in the plot for better clarity on missclassifications\n  for r in range(cmx.shape[0]):\n      for c in range(cmx.shape[1]):\n          ax.text(c, r, cmx[r,c], ha=\"center\", va=\"center\", color='white', fontsize=20)\n  fig.colorbar(img)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:40.895961Z","iopub.execute_input":"2021-06-05T14:31:40.896268Z","iopub.status.idle":"2021-06-05T14:31:41.061058Z","shell.execute_reply.started":"2021-06-05T14:31:40.896239Z","shell.execute_reply":"2021-06-05T14:31:41.060096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader\n\nWriting helper class for data extraction, tranformation and preprocessing  \nhttps://pytorch.org/docs/stable/data","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\nfrom sklearn.model_selection import train_test_split\n\nIMG_WIDTH=256\nIMG_HEIGHT=256\nclass Dataset(BaseDataset):\n    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n        \n    def __init__(\n            self, \n            images_dir=None, \n            masks_dir=None, \n            classes=None, \n            all_classes=None,\n            augmentation=None, \n            preprocessing=None,\n            index_list=None,\n            img_dims=(IMG_WIDTH,IMG_HEIGHT)\n    ):\n        self.all_classes=all_classes\n        if classes is None:\n          classes = all_classes\n        self.ids = []#list(range(len(index_list)))\n        # Load images and masks with progress updates printed\n        self.images=[]\n        self.masks_raw=[]\n        counter=1\n        for idx in index_list:\n          if masks_dir is not None:\n            mask_i = cv2.resize(np.load(masks_dir+'/train_{}.npy'.format(idx)),\n                  img_dims, interpolation = cv2.INTER_NEAREST)\n            found = False\n            for cls in classes:\n              if np.any(mask_i==all_classes.index(cls)+1):\n                self.masks_raw.append(mask_i)\n                self.ids.append(idx)\n                found=True\n                break\n            if not found:\n              continue\n          if \"train\" in images_dir:\n            self.images.append(cv2.resize(\n                  np.load(images_dir+'/train_{}.npy'.format(idx)),\n                  img_dims, interpolation = cv2.INTER_AREA))\n          else:\n            self.images.append(cv2.resize(\n                  np.load(images_dir+'/test_{}.npy'.format(idx)),\n                  img_dims, interpolation = cv2.INTER_AREA))\n          counter+=1\n          if (counter%100==0) or (counter==len(index_list)):\n            print(counter,\"/\",len(index_list),\"loaded.\")\n        #self.images=[np.load(images_dir+'/train_{}.npy'.format(idx)) for idx in index_list]\n        #self.masks_raw=[np.load(masks_dir+'/train_{}.npy'.format(idx)) for idx in index_list]\n        \n        # convert str names to class values on masks\n        self.class_values = [all_classes.index(cls.lower()) for cls in classes]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def get_original_image(self,i):\n      return self.images[i]\n\n\n    def __getitem__(self, i):\n        \n        # read data\n        image = self.images[i]\n        if len(self.masks_raw)>0:\n          mask = self.masks_raw[i]\n          # extract certain classes from mask (e.g. cars)\n          bg_values=list(range(len(self.all_classes)+1))\n          for v in self.class_values:\n            bg_values.remove(v+1)\n          bg_mask=np.zeros(mask.shape)\n          bg_mask[np.isin(mask,bg_values)]=1\n          masks = [(mask == v+1) for v in self.class_values]\n          mask = np.stack(masks, axis=-1).astype('float')\n          #print(bg_mask)\n          #print(mask.shape)\n          bg_mask = np.expand_dims(bg_mask, 2)\n          #print(bg_mask.shape)\n          mask=np.concatenate([bg_mask,mask],axis=2)\n        else:\n          mask=None\n        \n        \n        \n        # apply augmentations\n        if self.augmentation:\n            if len(self.masks_raw)>0:\n              sample = self.augmentation(image=image, mask=mask)\n              image, mask = sample['image'], sample['mask']\n            else:\n              image = self.augmentation(image=image)['image']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            if len(self.masks_raw)>0:\n              sample = self.preprocessing(image=image, mask=mask)\n              image, mask = sample['image'], sample['mask']\n            else:\n              image = self.preprocessing(image=image)['image']\n\n            \n        return image, mask\n        \n    def __len__(self):\n        return len(self.images)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:41.062467Z","iopub.execute_input":"2021-06-05T14:31:41.063037Z","iopub.status.idle":"2021-06-05T14:31:42.230757Z","shell.execute_reply.started":"2021-06-05T14:31:41.062995Z","shell.execute_reply":"2021-06-05T14:31:42.229972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Augmentations","metadata":{}},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n\n        albu.HorizontalFlip(p=0.5),\n\n        albu.ShiftScaleRotate(scale_limit=(-0.5,1), rotate_limit=0, shift_limit=0.1, p=0.2, border_mode=0), #1\n\n        #CopyPaste(blend=True, sigma=1, pct_objects_paste=0.8, max_paste_objects=1, p=1.), #pct_objects_paste is a guess\n        # it's a dual transform so doesn't work\n\n        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n        albu.RandomCrop(height=320, width=320, always_apply=True),\n\n\n        albu.IAAAdditiveGaussianNoise(p=0.1), #0.2\n        albu.IAAPerspective(p=0.1), #0.5\n\n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.RandomBrightness(p=1),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.2, #0.9\n        ),\n\n        albu.OneOf(\n            [\n                albu.IAASharpen(p=1),\n                albu.Blur(blur_limit=3, p=1),\n                albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.2, #0.9\n        ),\n\n        \n    ]\n    return albu.Compose(train_transform)\n'''\n\n        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n        albu.RandomCrop(height=320, width=320, always_apply=True),\n\n\n        albu.IAAAdditiveGaussianNoise(p=0.2),\n        albu.IAAPerspective(p=0.5),\n\n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.RandomBrightness(p=1),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.IAASharpen(p=1),\n                albu.Blur(blur_limit=3, p=1),\n                albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.RandomContrast(p=1),\n                albu.HueSaturationValue(p=1),\n            ],\n            p=0.9,\n        ),'''\n\ndef get_validation_augmentation():\n    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.PadIfNeeded(384, 480)\n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:42.23409Z","iopub.execute_input":"2021-06-05T14:31:42.234384Z","iopub.status.idle":"2021-06-05T14:31:42.246674Z","shell.execute_reply.started":"2021-06-05T14:31:42.234333Z","shell.execute_reply":"2021-06-05T14:31:42.244678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create model and train","metadata":{}},{"cell_type":"markdown","source":"### Load pretrained segmentation model\n\nWe used EfficientNet-B4 as the encoder/backbone for extracting features from the images and DeepLabV3Plus as the decoder head for labeling pixels.\nBoth were chosen on a trial and error basis after some theoretical deliberation. For the encoder we tried ResNeXt 50, MobileNet V2 and EfficientNet-B0 before chosing EfficientNet-B4 on the basis of accuracy and speed of training. We tried these encoders, because they have a relatively low number of parameters, which we expected to make model training easier on limited computational resources, and also less prone to overfitting on our relatively small data. As for the other models of the EfficientNet family, we only tried B0 and B4 because the former has the least amount of parameters and the latter seemed to have the best accuracy to model size ratio in the authors original publication.\n\nFor the decoder we tried FPN too, but DeepLabV3Plus worked much better. In the end we trained our final model for ~9000 epochs, reaching the best accuracy after ~6000 epochs, after which not even more augmentation and lower learning rate could give further improvements.","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n!pip show segmentation_models_pytorch\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:42.248255Z","iopub.execute_input":"2021-06-05T14:31:42.248872Z","iopub.status.idle":"2021-06-05T14:31:48.629277Z","shell.execute_reply.started":"2021-06-05T14:31:42.248824Z","shell.execute_reply":"2021-06-05T14:31:48.628477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENCODER = 'efficientnet-b4'\nENCODER_WEIGHTS = 'imagenet'\nALL_CLASSES = list(train_df.columns[:])\nCLASSES = labels #\nACTIVATION = \"softmax2d\" # could be None for logits or 'softmax2d' for multiclass segmentation\nDEVICE = 'cuda'\n\n# create segmentation model with pretrained encoder\n\"\"\"\nmodel = smp.DeepLabV3Plus(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES)+1, \n    activation=ACTIVATION,\n)\n\"\"\"\nMODEL_DIR='../input/trial-efficientnetb4-3-tpth/'\nmodel=torch.load(MODEL_DIR+'trial_efficientnetb4_3_T.pth')\n\n# Preprocessing specific to the used backbone CNN\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:47:55.790454Z","iopub.execute_input":"2021-06-03T18:47:55.790817Z","iopub.status.idle":"2021-06-03T18:48:01.367948Z","shell.execute_reply.started":"2021-06-03T18:47:55.790775Z","shell.execute_reply":"2021-06-03T18:48:01.367163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate the train/validation split\n\nWe split the data into train-validation sets with a 2-1 ratio. We also checked, that all the classes are present in both training and validation sets. Fortunately not only are there no missing classes in either groups, but the class distributions are very similar.","metadata":{}},{"cell_type":"code","source":"indexes=np.array((range(len(train_df))))\n\nX_train, X_valid= train_test_split(indexes, test_size=0.1, random_state=42)\n# if it is desired to test on labeled data, this data is taken from the validation set\nif do_test_split: \n  X_valid, X_test = train_test_split(X_valid, test_size=0.33, random_state=42)\nelse:\n  X_test =np.array((range(len(test_df))))\n\nprint(X_train.shape)\nprint(X_valid.shape)\nprint(X_test.shape)\n\ntrain_classes=train_df.iloc[X_train,:].sum(axis=0)/len(X_train)\nvalid_classes=train_df.iloc[X_valid,:].sum(axis=0)/len(X_valid)\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(20, 5))\nrects1 = ax.bar(x - width/2, train_classes, width, label='Training')\nrects2 = ax.bar(x + width/2, valid_classes, width, label='Validation')\n\nax.set_ylabel('Class relative frequencies')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\n\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:48:01.369226Z","iopub.execute_input":"2021-06-03T18:48:01.369567Z","iopub.status.idle":"2021-06-03T18:48:01.775226Z","shell.execute_reply.started":"2021-06-03T18:48:01.369531Z","shell.execute_reply":"2021-06-03T18:48:01.77453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the images\n\nThe images get loaded into the Dataset class, which are loaded into the DataLoader class handling the batches. Preprocessing and augmentation is applied later on in an on-line fashion during the training process.","metadata":{}},{"cell_type":"code","source":"train_dataset = Dataset(\n    x_train_dir, \n    y_train_dir, \n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES, #\n    all_classes=ALL_CLASSES,\n    index_list=X_train.tolist()\n)\n\nprint(\"train set: \",len(train_dataset))\n#for i in range(len(train_dataset)):\n  #im,m = train_dataset[i]\n  #print(m)\n\nvalid_dataset = Dataset(\n    x_train_dir,\n    y_train_dir,\n    augmentation=None,#get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n    all_classes=ALL_CLASSES,\n    index_list=X_valid.tolist()\n)\n\nprint(\"val set: \",len(valid_dataset))","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:48:01.776435Z","iopub.execute_input":"2021-06-03T18:48:01.776821Z","iopub.status.idle":"2021-06-03T18:48:21.228324Z","shell.execute_reply.started":"2021-06-03T18:48:01.776785Z","shell.execute_reply":"2021-06-03T18:48:21.227418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:48:21.229596Z","iopub.execute_input":"2021-06-03T18:48:21.230142Z","iopub.status.idle":"2021-06-03T18:48:21.235255Z","shell.execute_reply.started":"2021-06-03T18:48:21.230104Z","shell.execute_reply":"2021-06-03T18:48:21.234471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom thresholding for model accuracy evaluation\n\nBesides Jaccard loss for training the model, we were monitoring the IoU for accuracy.\nWhen classifying pixels, every pixel has to belong to a class, because the background is also treated as a class! However, for evaluating the loss or accuracy, we were ignoring the background pixels.\n\nTo better estimate IoU accuracy we decided to use a local threshold instead of a global one. In our case local threshold means that for every pixel we take the maximum probability class prediction and set it to 1, and all the other classes probabilities' to 0. A global threshold would be setting all class probabilities above a certain value (eg. 0.5) to 1 and to 0 below. The problem with this would be that every pixel can only be classified as one class in the end, and a threshold under 0.5 can give probability 1 to multiple classes, but a threshold of 0.5 or above can mean that a pixel doesnt get classified as anything, not even as background, especially with so many classes.\n\nUnfortunately, the segmentation library only had global thresholding, so we implemented a custom IoU function with local thresholding. ","metadata":{}},{"cell_type":"code","source":"# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index\n\n\ndef _take_channels(*xs, ignore_channels=None):\n  if ignore_channels is None:\n      return xs\n  else:\n      channels = [channel for channel in range(xs[0].shape[1]) if channel not in ignore_channels]\n      xs = [torch.index_select(x, dim=1, index=torch.tensor(channels).to(x.device)) for x in xs]\n      return xs\n\ndef _threshold(x, threshold=None):\n    if threshold is not None:\n        return (x > threshold).type(x.dtype)\n    else:\n        return x\n\ndef _threshold_local(x):\n    x_max = torch.argmax(x,1,keepdim=True)\n    x_ret = torch.zeros(x.shape, device='cuda:0').scatter(1,x_max,1.0)\n    #print(x_ret.shape)\n    return x_ret\n\ndef iou(pr, gt, eps=1e-7, threshold=None, ignore_channels=None):\n  pr = _threshold_local(pr)\n  pr, gt = _take_channels(pr, gt, ignore_channels=ignore_channels)\n\n  intersection = torch.sum(gt * pr)\n  union = torch.sum(gt) + torch.sum(pr) - intersection + eps\n  #print(\"intersection=\",intersection,\"union=\",union,\"gt=\",torch.sum(gt),\"pr=\",torch.sum(pr))\n  score = (intersection + eps) / union\n  '''\n  if score > 5:\n    print(pr.shape)\n    print(\"intersection=\",intersection,\"union=\",union,\"gt=\",torch.sum(gt),\"pr=\",torch.sum(pr),\"iou=\",score)\n    pr_sing = np.argmax(pr.cpu().numpy(),axis=1)\n    gt_sing = np.argmax(gt.cpu().numpy(),axis=1)\n    for j in range(pr.shape[1]):\n      visualize(pr=pr_sing[j,:,:], gt=gt_sing[j,:,:])\n    print(np.unique(pr_flat))\n    pr_flat = pr_sing.flatten()\n    gt_flat = gt_sing.flatten()\n    plot_confusion_matrix(gt_flat, pr_flat)\n    raise ValueError(\"done\")\n    '''\n  return score\n\n# This is the custom IoU metric used for the training\nclass IoU2(smp.utils.metrics.IoU):\n\n  def forward(self, y_pr, y_gt):\n    y_pr = self.activation(y_pr)\n    return iou(\n        y_pr, y_gt,\n        eps=1e-7,\n        threshold=self.threshold,\n        ignore_channels=self.ignore_channels,\n    )\n\nloss = smp.utils.losses.JaccardLoss(activation=ACTIVATION,ignore_channels=[0])\nmetrics2 = [IoU2(activation=ACTIVATION,ignore_channels=[0])]\n#metrics1 = [smp.utils.metrics.IoU(threshold=None,activation=ACTIVATION,ignore_channels=[0])]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=1e-4),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Epoch runners\n\nThe TrainEpoch and ValidationEpoch classes are the wrappers for running the training and evaluation epochs. Every epoch needs to be called by a loop.","metadata":{}},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics2, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=False,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics2, \n    device=DEVICE,\n    verbose=False,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:48:21.263801Z","iopub.execute_input":"2021-06-03T18:48:21.26403Z","iopub.status.idle":"2021-06-03T18:48:21.29226Z","shell.execute_reply.started":"2021-06-03T18:48:21.264008Z","shell.execute_reply":"2021-06-03T18:48:21.291661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start new logs\n#train_logs={\"jaccard_loss\":[],\"iou_score\":[]}\n#valid_logs={\"jaccard_loss\":[],\"iou_score\":[]}\n# Load logs from file\nf = open(\"../input/segmentation-logs/train_logs.pkl\", \"rb\")\ntrain_logs = pickle.load(f)\nf = open(\"../input/segmentation-logs/valid_logs.pkl\", \"rb\")\nvalid_logs = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:48:21.293788Z","iopub.execute_input":"2021-06-03T18:48:21.294015Z","iopub.status.idle":"2021-06-03T18:48:21.333891Z","shell.execute_reply.started":"2021-06-03T18:48:21.293993Z","shell.execute_reply":"2021-06-03T18:48:21.333062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert torch.cuda.is_available(), \"No GPU available\"\nimport time\nimport IPython\n\n\n\n# train model\n#max_score=0.4003615085446552\nmax_score=0.3871722652839199\n#MODEL_DIR='/output/working'\n\n#for j in range(5):\n  #model = torch.load(MODEL_DIR+'trial4_T.pth')\n  #create_epoch(model) ### This does not initialize train_epoch as maybe intended \n  #optimizer.param_groups[0]['lr'] = 1e-5\n# assign valid_log for the first iteration print\nvalid_log={\"iou_score\":0}\nout = display(IPython.display.Pretty('Start training'), display_id=True)\nfor i in range(8530, 8700):\n    if i == -1:\n        optimizer.param_groups[0]['lr'] = 1e-4\n        print('Decrease decoder learning rate to 1e-4!')\n            \n    out.update(IPython.display.Pretty(\n      'Epoch {it} training, previous validation score: {score:.4f}'.format(it=i,score=valid_log[\"iou_score\"])))\n    start = time.time()\n    train_log = train_epoch.run(train_loader)\n    end = time.time()\n    out.update(IPython.display.Pretty(\n      'Epoch {it} validation, training epoch took {dur:.2f}s'.format(it=i,dur=end-start)))\n    valid_log = valid_epoch.run(valid_loader)\n    # store loss and metric\n    for s in [\"jaccard_loss\",\"iou_score\"]:\n        train_logs[s].append(train_log[s])\n        valid_logs[s].append(valid_log[s])\n    # save model if validation score improved\n    if max_score < valid_log['iou_score']:\n        max_score = valid_log['iou_score']\n        torch.save(model, 'trial_efficientnetb4_3_T.pth')\n        print('Model saved! New best:',max_score)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-03T18:48:21.335531Z","iopub.execute_input":"2021-06-03T18:48:21.335783Z","iopub.status.idle":"2021-06-03T22:22:45.124343Z","shell.execute_reply.started":"2021-06-03T18:48:21.33576Z","shell.execute_reply":"2021-06-03T22:22:45.122086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save logs to file\nf = open(\"train_logs.pkl\",\"wb\")\npickle.dump(train_logs,f)\nf.close()\nf = open(\"valid_logs.pkl\",\"wb\")\npickle.dump(valid_logs,f)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:23:10.056307Z","iopub.execute_input":"2021-06-03T22:23:10.056624Z","iopub.status.idle":"2021-06-03T22:23:10.16628Z","shell.execute_reply.started":"2021-06-03T22:23:10.056596Z","shell.execute_reply":"2021-06-03T22:23:10.165501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model, 'trial_efficientnetb4_3_T.pth')","metadata":{"execution":{"iopub.status.busy":"2021-05-29T15:23:42.359942Z","iopub.execute_input":"2021-05-29T15:23:42.360267Z","iopub.status.idle":"2021-05-29T15:23:42.55947Z","shell.execute_reply.started":"2021-05-29T15:23:42.360239Z","shell.execute_reply":"2021-05-29T15:23:42.558685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(train_logs)\n#print(valid_logs)\n# summarize history for accuracy\nf,ax = plt.subplots(nrows=1,ncols=2,figsize=(20,5))\nax[0].plot(train_logs['jaccard_loss'])\nax[0].plot(valid_logs['jaccard_loss'])\nax[0].set_title('Loss progress')\nax[0].set_ylabel('Jaccard loss')\nax[0].set_xlabel('epoch')\nax[1].plot(train_logs['iou_score'])\nax[1].plot(valid_logs['iou_score'])\nax[1].set_title('Score progress')\nax[1].set_ylabel('IoU score')\nax[1].set_xlabel('epoch')\nf.legend(['training set', 'validation set'], loc='upper right')\nplt.suptitle('Model Performance Over Epochs',fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-03T22:22:51.103407Z","iopub.execute_input":"2021-06-03T22:22:51.103774Z","iopub.status.idle":"2021-06-03T22:22:51.455994Z","shell.execute_reply.started":"2021-06-03T22:22:51.10372Z","shell.execute_reply":"2021-06-03T22:22:51.454992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load best saved checkpoint\nbest_model = torch.load(MODEL_DIR+'trial_efficientnetb4_3_T.pth')\n# evaluate model on test set\ntest_vis_epoch = smp.utils.train.ValidEpoch(\n    model=best_model,\n    loss=loss,\n    metrics=metrics2,\n    device=DEVICE,\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T15:21:16.627453Z","iopub.execute_input":"2021-05-29T15:21:16.627811Z","iopub.status.idle":"2021-05-29T15:21:16.816087Z","shell.execute_reply.started":"2021-05-29T15:21:16.627781Z","shell.execute_reply":"2021-05-29T15:21:16.815312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation dataset without transformations for retrieving best IoU\ntest_dataset_vis = Dataset(\n    x_train_dir, y_train_dir,\n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES, #,\n    all_classes=ALL_CLASSES,\n    index_list=X_valid\n)\ntest_dataloader_vis = DataLoader(test_dataset_vis, batch_size=1, shuffle=False, num_workers=2)\nvis_log=test_vis_epoch.run(test_dataloader_vis)\nprint(vis_log[\"iou_score\"])\nprint(len(test_dataloader_vis))","metadata":{"execution":{"iopub.status.busy":"2021-05-29T15:21:19.373654Z","iopub.execute_input":"2021-05-29T15:21:19.373973Z","iopub.status.idle":"2021-05-29T15:21:23.206265Z","shell.execute_reply.started":"2021-05-29T15:21:19.373944Z","shell.execute_reply":"2021-05-29T15:21:23.205243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(75):\n  #n = np.random.choice(len(test_dataset_vis))\n  n=i\n  \n  \n  image, gt_mask = test_dataset_vis[n]\n  image_vis = test_dataset_vis.get_original_image(n)\n\n  image_vis = image_vis.astype('uint8')\n\n  x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0).float()\n\n  pr_mask = best_model.predict(x_tensor)\n  activation=torch.nn.Softmax(dim=1)\n  #pr_mask = activation(pr_mask)\n\n  gt_mask=torch.from_numpy(gt_mask).to(DEVICE).unsqueeze(0)\n\n  score=iou(pr_mask,gt_mask,threshold=None, ignore_channels=[0]).item()\n\n  cl_mask=pr_mask.argmax(1)\n  gt_mask=gt_mask.argmax(1)\n\n\n  cl_mask = cl_mask.squeeze().cpu().numpy()\n  gt_mask = gt_mask.squeeze().cpu().numpy()\n\n  if score >0:\n    #print(i)\n    if do_test_split:\n      visualize(\n        image=image_vis, \n        ground_truth_mask=gt_mask, \n        predicted_mask=cl_mask)\n    else:\n      plot_mask(\n        image_vis, \n        cl_mask,\n        gt_mask,\n        score=score,\n        labels=CLASSES\n      )","metadata":{"execution":{"iopub.status.busy":"2021-05-29T15:21:23.208263Z","iopub.execute_input":"2021-05-29T15:21:23.208649Z","iopub.status.idle":"2021-05-29T15:21:51.999795Z","shell.execute_reply.started":"2021-05-29T15:21:23.208589Z","shell.execute_reply":"2021-05-29T15:21:51.998972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Your Kaggle submission\nYour filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook.","metadata":{"papermill":{"duration":0.295407,"end_time":"2021-03-29T09:05:16.448937","exception":false,"start_time":"2021-03-29T09:05:16.15353","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def _rle_encode(img):\n    \"\"\"\n    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n\n    Parameters\n    ----------\n    img: np.ndarray - binary img array\n    \n    Returns\n    -------\n    rle: String - running length encoded version of img\n    \"\"\"\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    rle = ' '.join(str(x) for x in runs)\n    return rle\n\ndef generate_submission(df):\n    \"\"\"\n    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n    \n    Parameters\n    ----------\n    df: pd.DataFrame - filled dataframe that needs to be converted\n    \n    Returns\n    -------\n    submission_df: pd.DataFrame - df in submission format.\n    \"\"\"\n    df_dict = {\"Id\": [], \"Predicted\": []}\n    for idx, _ in df.iterrows():\n        df_dict[\"Id\"].append(f\"{idx}_classification\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n    \n    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n    submission_df.to_csv(\"submission.csv\")\n    return submission_df","metadata":{"papermill":{"duration":0.293321,"end_time":"2021-03-29T09:05:17.043036","exception":false,"start_time":"2021-03-29T09:05:16.749715","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Image classification\nThe goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe).","metadata":{"papermill":{"duration":0.261504,"end_time":"2021-03-29T09:05:17.563178","exception":false,"start_time":"2021-03-29T09:05:17.301674","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RandomClassificationModel:\n    \"\"\"\n    Random classification model: \n        - generates random labels for the inputs based on the class distribution observed during training\n        - assumes an input can have multiple labels\n    \"\"\"\n    def fit(self, X, y):\n        \"\"\"\n        Adjusts the class ratio variable to the one observed in y. \n\n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n        y: list of arrays - n x (nb_classes)\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.distribution = np.mean(y, axis=0)\n        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n        return self\n        \n    def predict(self, X):\n        \"\"\"\n        Predicts for each input a label.\n        \n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n            \n        Returns\n        -------\n        y_pred: list of arrays - n x (nb_classes)\n        \"\"\"\n        np.random.seed(0)\n        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n    \n    def __call__(self, X):\n        return self.predict(X)\n    \nmodel = RandomClassificationModel()\nmodel.fit(train_df[\"img\"], train_df[labels])\ntest_df.loc[:, labels] = model.predict(test_df[\"img\"])\ntest_df.head(1)","metadata":{"papermill":{"duration":3.860357,"end_time":"2021-03-29T09:05:21.741571","exception":false,"start_time":"2021-03-29T09:05:17.881214","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Semantic segmentation\nThe goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe).","metadata":{"papermill":{"duration":0.194695,"end_time":"2021-03-29T09:05:22.154564","exception":false,"start_time":"2021-03-29T09:05:21.959869","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RandomSegmentationModel:\n    \"\"\"\n    Random segmentation model: \n        - generates random label maps for the inputs based on the class distributions observed during training\n        - every pixel in an input can only have one label\n    \"\"\"\n    def fit(self, X, Y):\n        \"\"\"\n        Adjusts the class ratio variable to the one observed in Y. \n\n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n        Y: list of arrays - n x (height x width)\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n        print(\"Setting class distribution to:\\nbackground: {}\\n{}\".format(self.distribution[0], \"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution[1:]))))\n        return self\n        \n    def predict(self, X):\n        \"\"\"\n        Predicts for each input a label map.\n        \n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n            \n        Returns\n        -------\n        Y_pred: list of arrays - n x (height x width)\n        \"\"\"\n        np.random.seed(0)\n        return [np.random.choice(np.arange(len(labels) + 1), size=X_.shape[:2], p=self.distribution) for X_ in X]\n    \n    def __call__(self, X):\n        return self.predict(X)\n    \nmodel = RandomSegmentationModel()\nmodel.fit(train_df[\"img\"], train_df[\"seg\"])\ntest_df.loc[:, \"seg\"] = model.predict(test_df[\"img\"])\ntest_df.head(1)","metadata":{"papermill":{"duration":12.607971,"end_time":"2021-03-29T09:05:34.966182","exception":false,"start_time":"2021-03-29T09:05:22.358211","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit to competition\nYou don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details.","metadata":{"papermill":{"duration":0.193218,"end_time":"2021-03-29T09:05:35.361389","exception":false,"start_time":"2021-03-29T09:05:35.168171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"generate_submission(test_df)","metadata":{"papermill":{"duration":90.600773,"end_time":"2021-03-29T09:07:06.153425","exception":false,"start_time":"2021-03-29T09:05:35.552652","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Adversarial attack\nFor this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations.","metadata":{"papermill":{"duration":0.197143,"end_time":"2021-03-29T09:07:06.546217","exception":false,"start_time":"2021-03-29T09:07:06.349074","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install efficientnet_pytorch\n!pip install torchsummary\nfrom efficientnet_pytorch import EfficientNet\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nmodel = EfficientNet.from_pretrained('efficientnet-b4', num_classes=20)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:31:55.995191Z","iopub.execute_input":"2021-06-05T14:31:55.995565Z","iopub.status.idle":"2021-06-05T14:32:08.923737Z","shell.execute_reply.started":"2021-06-05T14:31:55.99553Z","shell.execute_reply":"2021-06-05T14:32:08.922568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n\n        albu.HorizontalFlip(p=0.5),\n\n        albu.ShiftScaleRotate(scale_limit=(-0.5,1), rotate_limit=0, shift_limit=0.1, p=0.2, border_mode=0), #1\n\n        #CopyPaste(blend=True, sigma=1, pct_objects_paste=0.8, max_paste_objects=1, p=1.), #pct_objects_paste is a guess\n        # it's a dual transform so doesn't work\n\n        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n        albu.RandomCrop(height=320, width=320, always_apply=True),\n\n\n        albu.IAAAdditiveGaussianNoise(p=0.1), #0.2\n        albu.IAAPerspective(p=0.1), #0.5\n\n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.RandomBrightness(p=1),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.2, #0.9\n        ),\n\n        albu.OneOf(\n            [\n                albu.IAASharpen(p=1),\n                albu.Blur(blur_limit=3, p=1),\n                albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.2, #0.9\n        ),\n\n        \n    ]\n    return albu.Compose(train_transform)\n'''\n\n        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n        albu.RandomCrop(height=320, width=320, always_apply=True),\n\n\n        albu.IAAAdditiveGaussianNoise(p=0.2),\n        albu.IAAPerspective(p=0.5),\n\n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.RandomBrightness(p=1),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.IAASharpen(p=1),\n                albu.Blur(blur_limit=3, p=1),\n                albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.RandomContrast(p=1),\n                albu.HueSaturationValue(p=1),\n            ],\n            p=0.9,\n        ),'''\n\ndef get_validation_augmentation():\n    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.PadIfNeeded(384, 480)\n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:33:01.248088Z","iopub.execute_input":"2021-06-05T14:33:01.248434Z","iopub.status.idle":"2021-06-05T14:33:01.259116Z","shell.execute_reply.started":"2021-06-05T14:33:01.248399Z","shell.execute_reply":"2021-06-05T14:33:01.258263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_WIDTH=380\nIMG_HEIGHT=380\n\nclass ClassificationDataset(BaseDataset):\n    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n    \n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n        \n    def __init__(\n                self, \n                images_dir_path=None,\n                targets_path='/kaggle/input/kul-h02a5a-computervision-groupassignment1/train/train_set.csv',\n                index_list=None,\n                img_dims=(IMG_WIDTH,IMG_HEIGHT),\n                preprocessing=True,\n                augmentation=False\n                \n    ):\n        # list of numpy tensors\n        self.images=[]\n        # load targets based on indices (from train/validation split) into a numpy array\n        self.targets=np.take(pd.read_csv(targets_path,index_col=\"Id\").to_numpy(),index_list, axis=0)\n        \n        counter=1\n        for idx in index_list:\n            if \"train\" in images_dir_path:\n                self.images.append(cv2.resize(\n                      np.load(images_dir_path+'/train_{}.npy'.format(idx)),\n                      img_dims, interpolation = cv2.INTER_AREA))\n            else:\n                self.images.append(cv2.resize(\n                      np.load(images_dir_path+'/test_{}.npy'.format(idx)),\n                      img_dims, interpolation = cv2.INTER_AREA))\n            counter+=1\n            if (counter%100==0) or (counter==len(index_list)):\n                print(counter,\"/\",len(index_list),\"loaded.\")\n        #self.images=[np.load(images_dir+'/train_{}.npy'.format(idx)) for idx in index_list]\n        #self.masks_raw=[np.load(masks_dir+'/train_{}.npy'.format(idx)) for idx in index_list]\n                \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def get_original_image(self,i):\n        return self.images[i]\n\n\n    def __getitem__(self, i): \n        # self.images is a list of numpy tensors\n        image = self.images[i]\n        # self.targets is a numpy array\n        target=self.targets[i,:]\n        \n        # apply augmentations\n        if self.augmentation:\n            image = self.augmentation(image=image)['image']\n        # apply preprocessing\n        if self.preprocessing:\n            # Preprocess image\n            tfms = transforms.Compose([transforms.Resize(IMG_WIDTH), transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])\n\n            image = tfms(Image.fromarray(image)).unsqueeze(0)   \n        return {\"image\": image,\"targets\": target}\n        \n    def __len__(self):\n        return len(self.images)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:36:53.293666Z","iopub.execute_input":"2021-06-05T14:36:53.294017Z","iopub.status.idle":"2021-06-05T14:36:53.306868Z","shell.execute_reply.started":"2021-06-05T14:36:53.293984Z","shell.execute_reply":"2021-06-05T14:36:53.305732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, model, optimizer, loss_fun, device):\n    \n    model.train()\n    \n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        inputs = data[\"image\"]\n        targets = data['targets']\n        \n        inputs = inputs.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = loss_fun\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:36:57.229633Z","iopub.execute_input":"2021-06-05T14:36:57.229955Z","iopub.status.idle":"2021-06-05T14:36:57.236045Z","shell.execute_reply.started":"2021-06-05T14:36:57.229923Z","shell.execute_reply":"2021-06-05T14:36:57.234739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"indexes=np.array((range(len(train_df))))\n\nX_train, X_valid= train_test_split(indexes, test_size=0.1, random_state=42)\n# if it is desired to test on labeled data, this data is taken from the validation set\nif do_test_split: \n    X_valid, X_test = train_test_split(X_valid, test_size=0.33, random_state=42)\nelse:\n    X_test =np.array((range(len(test_df))))\n\nprint(X_train.shape)\nprint(X_valid.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:36:59.019573Z","iopub.execute_input":"2021-06-05T14:36:59.019907Z","iopub.status.idle":"2021-06-05T14:36:59.028223Z","shell.execute_reply.started":"2021-06-05T14:36:59.019874Z","shell.execute_reply":"2021-06-05T14:36:59.027403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ClassificationDataset(\n    x_train_dir, \n    '/kaggle/input/kul-h02a5a-computervision-groupassignment1/train/train_set.csv',\n    index_list=X_train.tolist(),\n    preprocessing=True,\n    augmentation=get_training_augmentation(),    \n)\n\nprint(\"train set: \",len(train_dataset))\n#for i in range(len(train_dataset)):\n  #im,m = train_dataset[i]\n  #print(m)\n\nvalid_dataset = ClassificationDataset(\n    x_train_dir, \n    '/kaggle/input/kul-h02a5a-computervision-groupassignment1/train/train_set.csv',\n    index_list=X_valid.tolist(),\n    preprocessing=True,\n    augmentation=False \n)\n\nprint(\"val set: \",len(valid_dataset))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:37:01.007815Z","iopub.execute_input":"2021-06-05T14:37:01.008132Z","iopub.status.idle":"2021-06-05T14:37:02.396457Z","shell.execute_reply.started":"2021-06-05T14:37:01.008101Z","shell.execute_reply":"2021-06-05T14:37:02.39569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:37:04.889796Z","iopub.execute_input":"2021-06-05T14:37:04.890214Z","iopub.status.idle":"2021-06-05T14:37:04.9038Z","shell.execute_reply.started":"2021-06-05T14:37:04.890172Z","shell.execute_reply":"2021-06-05T14:37:04.902698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=1e-4),\n])\nloss_fun=torch.nn.CrossEntropyLoss()\nDEVICE = 'cuda'\n\nmodel.to(DEVICE)\n\nfor epoch in range(10):\n    train(train_loader, model, optimizer, loss_fun, device=DEVICE)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T14:37:06.850181Z","iopub.execute_input":"2021-06-05T14:37:06.850557Z","iopub.status.idle":"2021-06-05T14:37:07.683482Z","shell.execute_reply.started":"2021-06-05T14:37:06.850522Z","shell.execute_reply":"2021-06-05T14:37:07.681307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Discussion\nFinally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision.","metadata":{"papermill":{"duration":0.195133,"end_time":"2021-03-29T09:07:06.938031","exception":false,"start_time":"2021-03-29T09:07:06.742898","status":"completed"},"tags":[]}}]}